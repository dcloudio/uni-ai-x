import * as vsctm from '../../common/uni-textmate/main';
import onig from '../app-js/onig/main.js';
// import javascriptGrammar from '../../static/grammar/javascript.tmLanguage.json';


export interface IToken {
	readonly startIndex: number;
	readonly endIndex: number;
	readonly scopes: string[];
}
export interface ILineTokens{
	readonly tokens:IToken[],
	state:any
}

export type HighLighterOptions = {
	languages: Record<string, any>
}

export type CreateHighLighterRes = {
	tokenizeFullText(langId: string, fullCodeText:string): Promise<ILineTokens[]>
	tokenizeLine(langId: string, lineText: string, state: any): Promise<ILineTokens | null>
}

async function loadOnigWasmFile(): Promise<ArrayBuffer> {
	return new Promise((resolve, reject) =>{
		uni.request({
		  url: '/uni_modules/uni-highlight/static/_onig.wasm',
		  method: 'GET',
		  responseType: 'arraybuffer',
		  success: (res) => {
			console.log(res)
			resolve(res.data as ArrayBuffer);
		  },
		  fail: (err) => {
			console.error('读取失败', err);
			reject(err)
		  }
		})
	})
	
}

export async function createHighLighter(options: HighLighterOptions):Promise<CreateHighLighterRes> {
	const languages = options.languages;
	const langMap: Map<string, string> = new Map();
	const scopeMap: Map<string, object> = new Map();
	
	for(let id in languages){
		langMap.set(id, languages[id]?.scopeName)
		scopeMap.set(languages[id].scopeName, languages[id])
	}
	
	let wasmFile = await loadOnigWasmFile()
	
	const vscodeOnigurumaLib = onig.loadWASM(wasmFile).then(() => {
		return {
			createOnigScanner(patterns) { return new onig.OnigScanner(patterns); },
			createOnigString(s) { return new onig.OnigString(s); }
		};
	});
	const registry = new vsctm.Registry({
		onigLib: vscodeOnigurumaLib,
		loadGrammar: (scopeName) => {
			try {
				let content = JSON.stringify(scopeMap.get(scopeName))
				const filePath = "/static/javascript.tmLanguage.json";
				return Promise.resolve(vsctm.parseRawGrammar(content, filePath));
			} catch {
				return Promise.reject(null)
			}
			
			
		}
	});
	
	return {
		async tokenizeFullText(langId: string, fullCodeText:string): Promise<ILineTokens[]> {
			let scopeName = langMap.get(langId)
			if (!scopeName) return []
			const grammar = await registry.loadGrammar(scopeName)
			const text = fullCodeText.split(/\r\n|\r|\n/);
			let ruleStack = vsctm.INITIAL;
			let res: ILineTokens[] = []
			for (let i = 0; i < text.length; i++) {
				const line = text[i] + "\n";
				const lineTokens = grammar?.tokenizeLine(line, ruleStack);
				if (!lineTokens) return []
				const tokens:IToken[] = []
				for (let j = 0; j < lineTokens.tokens.length; j++) {
					const token = lineTokens.tokens[j];
					const iToken: IToken = {
						startIndex: token.startIndex,
						endIndex: token.endIndex,
						scopes: token.scopes
					}
					tokens.push(iToken)
				}
				res.push({
					tokens,
					state: lineTokens.ruleStack
				})
				ruleStack = lineTokens.ruleStack;
			}
			
			return res
		},
		async tokenizeLine(langId: string, lineText: string, state: any): Promise<ILineTokens | undefined> {
			let scopeName = langMap.get(langId)
			if (!scopeName) return
			let ruleStack = state ? state : vsctm.INITIAL;
			const grammar = await registry.loadGrammar(scopeName)
			const lineTokens = grammar?.tokenizeLine(lineText, ruleStack);
			if (!lineTokens) return
			const tokens:IToken[] = []
			for (let j = 0; j < lineTokens.tokens.length; j++) {
				const token = lineTokens.tokens[j];
				const iToken: IToken = {
					startIndex: token.startIndex,
					endIndex: token.endIndex,
					scopes: token.scopes
				}
				tokens.push(iToken)
			}
			return {
				tokens,
				state: lineTokens.ruleStack
			}
		}
	}
}